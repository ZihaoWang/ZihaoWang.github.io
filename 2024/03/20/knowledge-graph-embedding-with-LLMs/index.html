<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>Zihao Wang&#39;s Portfolio</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="MotivationIn the real world, the text associated with an entity may have multiple segments, each carrying different information. For example, consider the multi-segment text description from the entit">
<meta property="og:type" content="article">
<meta property="og:title" content="Zihao Wang&#39;s Portfolio">
<meta property="og:url" content="https://zihaowang.github.io/2024/03/20/knowledge-graph-embedding-with-LLMs/index.html">
<meta property="og:site_name" content="Zihao Wang&#39;s Portfolio">
<meta property="og:description" content="MotivationIn the real world, the text associated with an entity may have multiple segments, each carrying different information. For example, consider the multi-segment text description from the entit">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://zihaowang.github.io/images/g2cn1.png">
<meta property="og:image" content="https://zihaowang.github.io/images/g2cn2.png">
<meta property="og:image" content="https://zihaowang.github.io/images/g2cn3.png">
<meta property="article:published_time" content="2024-03-20T19:49:59.327Z">
<meta property="article:modified_time" content="2024-03-20T19:49:59.327Z">
<meta property="article:author" content="Zihao Wang">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://zihaowang.github.io/images/g2cn1.png">
  
    <link rel="alternate" href="/atom.xml" title="Zihao Wang's Portfolio" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<meta name="generator" content="Hexo 7.1.1"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Zihao Wang&#39;s Portfolio</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://zihaowang.github.io"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-knowledge-graph-embedding-with-LLMs" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/03/20/knowledge-graph-embedding-with-LLMs/" class="article-date">
  <time class="dt-published" datetime="2024-03-20T19:49:59.327Z" itemprop="datePublished">2024-03-20</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p><img src="/images/g2cn1.png"><br>In the real world, the text associated with an entity may have multiple segments, each carrying different information. For example, consider the multi-segment text description from the entity “Python” in the above figure. A model should extract information from the third segment for “Who invented the Python language?”. For the question “Why Python is a popular language?”, the model should focus on the first and the second segments.<br>Therefore, when encoding multi-segment texts into embeddings, we should jointly consider these segments and model their interactions.</p>
<h2 id="Approach-1-Embeddings-from-LLMs"><a href="#Approach-1-Embeddings-from-LLMs" class="headerlink" title="Approach 1: Embeddings from LLMs"></a>Approach 1: Embeddings from LLMs</h2><p>We present G2CN, an extension of graph convolutional networks (GCNs), as the knowledge graph embedding method.<br>As our previous work, “Question Answering with KG and language models,” G2CN uses hypercomplex algebra to integrate multiple embeddings from text segments, where we extract these embeddings from LLMs, and each embedding corresponds to a segment. Upon our G2CN, we employ different networks for diverse downstream tasks:</p>
<ol>
<li>Node classification (NC) classifies the label of entities. In our case, the label is the type of the entity, e.g., the label “Python” is “programming language.</li>
<li>Link prediction (LP) judges if a connection exists between two entities in the knowledge graph. For example, “Python” and “C++” have a connection, but “Python” and “Barack Obama” do not.</li>
</ol>
<p>We conduct experiments on two citation KGs, PubmedText and ArxivText, where papers are nodes and references are edges. For both datasets, we collect each paper’s title, abstract, keywords, and introduction as multiple segments of textual features and the category of papers as labels.<br>For evaluation metrics, we use the micro-F1 score for node classification, the area under the Receiver Operating Characteristic (ROC) curve, and the Average Precision (AP) score for link prediction. To ensure a fair comparison, we concatenate all text embeddings of each entity from the LLM for baselines without using hypercomplex algebras.<br><img src="/images/g2cn2.png"><br>From figure above, we demonstrate the NC and LP results using embeddings from the GPT3.5. Our G2CN demonstrates superior performance to other baselines on both tasks, achieving the best results across 5 of the 6 metrics. Using multiple hypercomplex geometric spaces proves beneficial.</p>
<h2 id="Approach-2-Prompt-based-methods-with-LLMs"><a href="#Approach-2-Prompt-based-methods-with-LLMs" class="headerlink" title="Approach 2: Prompt-based methods with LLMs"></a>Approach 2: Prompt-based methods with LLMs</h2><p>We also explore a prompt-based method with GPT3.5; the procedure is described as follows:</p>
<ol>
<li>We use multiple text segments of each entity as contexts of the prompt, and contexts are further split into trunks of 1024 words to fit the need of GPT3.5 API.</li>
<li>We construct a query prompt for each node in the test set with the context mentioned before, and the set of possible labels serves as the candidate targets. Specifically, we use the following prompts for the NC and LP tasks:<br>NC: You are a helpful AI assistant. Use the following pieces of context to predict the category. The task is to look at the contexts and to predict the category, which is a numerical value. If the category can not be predicted, then provide “unknown”.<br>LP: You are a helpful AI assistant. Use the following pieces of context to predict the Label: 1 or 0 based on “label: has connection” between two papers. The task is to look at the contexts and to predict the category, which should be a numerical value (0 or 1). 1 means a connection, and 0 means no connection. If the category can not be predicted, then provide “-1”.<br><img src="/images/g2cn3.png"><br>The results are shown in figure, where we demonstrate the NC and LP results using a prompt-based method with the GPT3.5. By comparing the results of the prompt-based method with other GNN-based models using GPT3.5 embeddings, we observe that the performance of the prompt-based method is inferior to other baselines and our model. This is because the prompt-based method cannot sufficiently utilize the interaction between multiple text segments of each entity, and the graph structure of the citation networks is ignored.</li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://zihaowang.github.io/2024/03/20/knowledge-graph-embedding-with-LLMs/" data-id="clu083cow0001k1f7gozm2uca" data-title="" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
  
    <a href="/2024/03/20/knowledge-graph-question-answering/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title"></div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/03/">March 2024</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2024/03/20/knowledge-graph-embedding-with-LLMs/">(no title)</a>
          </li>
        
          <li>
            <a href="/2024/03/20/knowledge-graph-question-answering/">(no title)</a>
          </li>
        
          <li>
            <a href="/2024/03/20/abstractive-text-summarization/">(no title)</a>
          </li>
        
          <li>
            <a href="/2024/03/20/modeling-uncommon-knowledge/">(no title)</a>
          </li>
        
          <li>
            <a href="/2024/03/20/recommendation-systems/">(no title)</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2024 Zihao Wang<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>