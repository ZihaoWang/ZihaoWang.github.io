<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>Knowledge Graph Question Answering Systems | Zihao Wang&#39;s Portfolio</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="Knowledge Graph Question Answering (KGQA) is an NLP task that focuses on developing systems capable of answering natural-language questions from humans using knowledge graphs. A knowledge graph (KG) i">
<meta property="og:type" content="article">
<meta property="og:title" content="Knowledge Graph Question Answering Systems">
<meta property="og:url" content="https://zihaowang.github.io/2024/03/21/knowledge-graph-question-answering/index.html">
<meta property="og:site_name" content="Zihao Wang&#39;s Portfolio">
<meta property="og:description" content="Knowledge Graph Question Answering (KGQA) is an NLP task that focuses on developing systems capable of answering natural-language questions from humans using knowledge graphs. A knowledge graph (KG) i">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://zihaowang.github.io/images/home1.jpg">
<meta property="og:image" content="https://zihaowang.github.io/images/multi_lm1.png">
<meta property="og:image" content="https://zihaowang.github.io/images/multi_lm2.png">
<meta property="og:image" content="https://zihaowang.github.io/images/multi_lm4.png">
<meta property="og:image" content="https://zihaowang.github.io/images/multi_lm5.png">
<meta property="og:image" content="https://zihaowang.github.io/images/multi_lm6.png">
<meta property="article:published_time" content="2024-03-21T14:16:54.890Z">
<meta property="article:modified_time" content="2024-03-21T14:16:54.890Z">
<meta property="article:author" content="Zihao Wang">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://zihaowang.github.io/images/home1.jpg">
  
    <link rel="alternate" href="/atom.xml" title="Zihao Wang's Portfolio" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<meta name="generator" content="Hexo 7.1.1"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Zihao Wang&#39;s Portfolio</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://zihaowang.github.io"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-knowledge-graph-question-answering" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/03/21/knowledge-graph-question-answering/" class="article-date">
  <time class="dt-published" datetime="2024-03-21T14:16:54.890Z" itemprop="datePublished">2024-03-21</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      Knowledge Graph Question Answering Systems
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p><strong>Knowledge Graph Question Answering (KGQA)</strong> is an NLP task that focuses on developing systems capable of answering natural-language questions from humans using knowledge graphs. A <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Knowledge_graph">knowledge graph</a> (KG) is a structured representation of information, typically entities, relations, and attributes. Below is an illustration of KGQA systems <a target="_blank" rel="noopener" href="https://medium.com/analytics-vidhya/open-domain-question-answering-series-part-3-introduction-to-knowledge-graphs-for-question-5d3f8d78812e">online</a>.”<br><img src="/images/home1.jpg"><br>Real-world KGs are highly incomplete, they may not contain all the information required to answer a given question. Texts, such as entity descriptions, documents, or other textual sources, provide additional context and details that might be missing in the structured KG.<br>Then, <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Language_model">Language models</a> play a crucial role in KGQA by providing natural language understanding, semantic representation, and adaptability to integrate texts with KGs.<br>In this project, we incorporate multiple language models for capturing word, sentence, and document levels of semantics from texts.<br>You may find our <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2208.02743">paper</a> and open-source code on <a target="_blank" rel="noopener" href="https://github.com/ZihaoWang/Hypercomplex-KG-Embedding">GitHub</a>.</p>
<h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>Knowledge graphs (KGs) are often incomplete and lack certain facts or relations. Knowledge Graph Question Answering (KGQA) systems face challenges when answering questions based on incomplete information. Missing facts can lead to inaccuracies or limitations in the generated answers.<br>When answering questions, this incomplete can be mitigated by providing additional textual information about entities or relations.<br><img src="/images/multi_lm1.png"><br>Above is a part of Wikidata. We assume that the purple dashed edge (Q5220733, P19, Q621549) is unknown because the entity “Q5220733” is only connected to one other entity, “Q193592”. Although facts cannot help bridge the gap between “Q5220733” and “Q621549”, additional entity descriptions from Wikipedia could be used to provide a solution.</p>
<p>Now, the question is how to encode entity descriptions into embeddings, and recent language models have become a feasible choice.<br>Previous KGQA systems, such as <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1909.03193">KG-BERT</a>, <a target="_blank" rel="noopener" href="https://aclanthology.org/2021.tacl-1.11.pdf">Kepler</a>, only represent texts with a single language model and may lead to inferior performance, because different language models excel in extracting different levels of information.<br>For example, <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1908.10084">SentenceTransformer</a> excels in extracting word and sentence-level information, <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1405.4053">Doc2Vec</a> mainly captures the document-level information.</p>
<h2 id="Our-Approach"><a href="#Our-Approach" class="headerlink" title="Our Approach"></a>Our Approach</h2><p>In this project, we incorporate multiple pre-trained language models for capturing word, sentence, and document levels of semantics from texts. Our approach can be viewed in the following figure, where we considers various entity representations, such as word, sentence, and document levels of information, and maps them into a joint geometric space using the hypercomplex algebra such as the Quaternion.<br><img src="/images/multi_lm2.png"></p>
<ol>
<li>We transform a question such as “Where was Danny Pena born?” into a query forming with the corresponding entity and relation in the question, e.g., (Q5220733, P19, ?). The task is to infer the missing answering entity in the query (the correct answering entity is Q621549).</li>
<li>For the query and candidate answering entities in the KG, we encode word, sentence, and document embeddings of entity descriptions with a variety of pre-trained language models: <strong>Word2Vec, FastText, Doc2Vec, SentenceTransformer</strong>. To simplify, we use the abbreviations <strong>{W, F, D, S}</strong> to denote these language models. </li>
<li>For each query and candidate answering entity, we integrate its multiple embeddings using hypercomplex algebra, which measures the pair-wise interaction between any two embeddings.</li>
<li>Finally, we calculate each candidate answering entity’s distance to the query in the hyperbolic space.<br>Based on different language models and ways of computing distances between the query and answering entities, we proposed three variants of our model:</li>
</ol>
<ul>
<li>Robin: uses <strong>one</strong> language model, and the distance is computed using <strong>rotation and translation</strong> in the hypercomplex space.</li>
<li>Lion: uses <strong>two</strong> language models, and the distance is also computed using <strong>rotation and translations</strong> like Robin.</li>
<li>Tetra: uses <strong>at most three</strong> language models, and the distance is computed <strong>only using rotation</strong>.</li>
</ul>
<h2 id="Question-Answering-Experiment"><a href="#Question-Answering-Experiment" class="headerlink" title="Question Answering Experiment"></a>Question Answering Experiment</h2><p>We evaluate our approach on two domain-specific KGQA datasets: Nations and Diabetes, and two commonsense KGQA datasets: FB15k-237, and YAGO-10:</p>
<ol>
<li><a target="_blank" rel="noopener" href="https://homes.cs.washington.edu/~pedrod/papers/mlc07.pdf">Nations</a> is a small-scale KG that consists of relation between countries.</li>
<li><a target="_blank" rel="noopener" href="https://academic.oup.com/bioinformatics/article/38/8/2235/6527626">Diabetes</a> is a medical KG specified for various diabetes.</li>
<li><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/document/8047276">FB15k-237</a> is a subset of the large-scale Freebase KG.</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1902.10197">YAGO-10</a> is a subset of the large-scale YAGO KG.</li>
</ol>
<p>We conducted a KGQA experiment, and the results can be viewed in the following Figure, which shows Question Answering results for both low (D&#x3D;32) and high (D&#x3D;500) dimension of embedding sizes.<br><img src="/images/multi_lm4.png"><br> For each dataset under different dimension sizes, numbers with the blue background are the best results. The category “Ours” includes three variants of our model with different combinations of language models. Specifically, <strong>“W, F, S, D”</strong> in the postfix means <strong>W</strong>ord2Vec, <strong>F</strong>astText, <strong>S</strong>entence Transformer, and <strong>D</strong>oc2Vec. ‘<br>We measure the rank of the correct answering entity among all candidates, just like how we evaluate the search engine! Specifically, we use two ranking-based metrics: <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Mean_reciprocal_rank">MRR</a> and <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Knowledge_graph_embedding#Hits@K">Hits@K</a>. Intuitively, MRR measures the averaging reciprocal rank of correct predictions, and Hits@K measures the proportion of correct predictions ranking in the top-K positions. <strong>For both metrics, the higher, the better.</strong><br>And here, we can obtain some conclusions from the results:</p>
<ol>
<li>On smaller datasets such as Nations, the amount of KG information is limited, and incorporating additional textual information becomes more crucial. Thus, models incorporating more language models perform better.</li>
<li>For multiple text embeddings from language models, modeling their pair-wise interactions is better than just concatenating them into a larger embedding, as models in the category of Baseline.</li>
</ol>
<h2 id="Semantic-Clusters-Of-Entity-Embedding"><a href="#Semantic-Clusters-Of-Entity-Embedding" class="headerlink" title="Semantic Clusters Of Entity Embedding"></a>Semantic Clusters Of Entity Embedding</h2><p>We analyze the semantics of learned entity embeddings from our approach. The goal is to determine if semantically similar entities also have similar representations in the learned embedding space. After dimensional reduction, the learned entity embeddings are shown in the following Figure.<br><img src="/images/multi_lm5.png"><br>In the figure, each cluster in the figure represents a particular topic, each point corresponds to an entity in the dataset, and each red arrow points to the corresponding point of an entity name.<br>We can see that semantically related entities are indeed clustered together in the learned embedding space. For instance, universities and languages are separated into distinct blue and red clusters.<br>This suggests our model can capture meaningful semantic relationships between entities in the KG.</p>
<h2 id="Explanation-for-question-answering-processes"><a href="#Explanation-for-question-answering-processes" class="headerlink" title="Explanation for question answering processes"></a>Explanation for question answering processes</h2><p>In this demo, we aim to investigate how our approach leverages sentence-level information by selecting the embedding of important sentences.<br>The figure below is an example of sentence contributions when answering questions, where a sentence with higher rank (smaller number) is more important. The source of the sentence indicates whether a sentence comes from the description of a head or tail entity.<br><img src="/images/multi_lm6.png"></p>
<ul>
<li>For the first question, “Which film is created by Mars Callahan?”, the top-ranked sentence comes from the description of the answering entity. It contains the keywords “directed by Mars Callahan,” which refers to the head entity.<br>For the second question, “Whom did Margaret Of Geneva marry?”, the top-1 and top-2 sentences refer to the information in question, “Margaret Of Geneva,” and the answering entity, “Thomas Count Of Savoy.” Notably, the keywords “escorting” and “carried her off” in the top sentence provide information about the relation “marry to.”</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://zihaowang.github.io/2024/03/21/knowledge-graph-question-answering/" data-id="clu083coy0002k1f70jlme10m" data-title="Knowledge Graph Question Answering Systems" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2024/03/21/modeling-uncommon-knowledge/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          Uncommon Knowledge in Vector Databases
        
      </div>
    </a>
  
  
    <a href="/2024/03/21/knowledge-graph-embedding-with-LLMs/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Knowledge Graph Embeddings with LLMs</div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/03/">March 2024</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2024/03/21/modeling-uncommon-knowledge/">Uncommon Knowledge in Vector Databases</a>
          </li>
        
          <li>
            <a href="/2024/03/21/knowledge-graph-question-answering/">Knowledge Graph Question Answering Systems</a>
          </li>
        
          <li>
            <a href="/2024/03/21/knowledge-graph-embedding-with-LLMs/">Knowledge Graph Embeddings with LLMs</a>
          </li>
        
          <li>
            <a href="/2024/03/21/recommendation-systems/">Automatic Text Summarization</a>
          </li>
        
          <li>
            <a href="/2024/03/21/abstractive-text-summarization/">(no title)</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2024 Zihao Wang<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>